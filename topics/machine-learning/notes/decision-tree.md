# 🌳 결정 트리 (Decision Tree) — 분할 기준, 정보이득, 가지치기

데이터를 반복적으로 분할하며 예측 규칙을 학습하는 **비모수(non-parametric)** 모델입니다.

---

## 🎯 분할 기준 (Splitting Criteria)

노드 \(t\)에서 특성 \(X_j\)를 기준값 \(s\)로 분할할 때, 왼쪽 자식 \(t_L\)과 오른쪽 자식 \(t_R\)의 불순도를 합한 **가중치 평균**을 최소화하는 방향으로 선택합니다.

\[
\text{SplitScore}(j,s)
= \frac{N_L}{N_t} \, I(t_L) \;+\; \frac{N_R}{N_t} \, I(t_R)
\]
- \(N_t\): 노드 \(t\)의 샘플 수  
- \(N_L, N_R\): 분할 후 왼/오 자식 노드 샘플 수  
- \(I(\cdot)\): 불순도(impurity) 지표  

### 🔍 분류용 불순도 지표

| 지표                      | 수식                                                                  | 특성                         |
|---------------------------|-----------------------------------------------------------------------|------------------------------|
| **엔트로피 (Entropy)**   | \(\displaystyle -\sum_{k=1}^K p_{k}\log_2 p_{k}\)                       | 정보 이론 기반, 부드러운 경사 |
| **지니 지수 (Gini)**      | \(\displaystyle \sum_{k=1}^K p_{k}(1-p_{k}) = 1 - \sum_{k=1}^K p_{k}^2\) | 계산 빠름, 트리 깊이 ↓ 효과  |
| **오분류율 (Misclassification Rate)** | \(\displaystyle 1 - \max_{k} p_{k}\)                                       | 직관적이나 민감도 낮음       |

- \(p_k\): 노드 내 클래스 \(k\) 비율  

### 🔍 회귀용 불순도 지표

| 지표                 | 수식                                               | 특성                         |
|----------------------|----------------------------------------------------|------------------------------|
| **MSE (감쇠총제곱오차)**   | \(\displaystyle \frac{1}{N_t}\sum_{i\in t}(y_i - \bar y_t)^2\) | 연속값 예측에 적합            |
| **MAE (평균절댓오차)**      | \(\displaystyle \frac{1}{N_t}\sum_{i\in t}|y_i - \bar y_t|\)   | 이상치에 강건               |

---

## 🔗 정보이득 (Information Gain)

**정보이득(Information Gain, IG)** 은 분할 전후 엔트로피 차이로 정의합니다.

\[
\mathrm{IG}(t, j, s)
= I(t) \;-\; \frac{N_L}{N_t}I(t_L) \;-\; \frac{N_R}{N_t}I(t_R)
\]
- \(I(\cdot)\): 보통 엔트로피  
- IG가 클수록 “순수도(purity)”가 크게 증가한 분할

---

## ✂️ 가지치기 (Pruning)

과적합을 방지하기 위해 불필요하게 복잡한 가지(branch)를 제거합니다.

### 1️⃣ 사전 가지치기 (Pre-Pruning)

| 기법                            | 설명                                                          |
|---------------------------------|---------------------------------------------------------------|
| **최대 깊이 제한 (max_depth)**  | 트리 최대 깊이 \(\le\) 지정 값                                |
| **최소 샘플 분할 (min_samples_split)** | 노드를 분할하기 위한 최소 샘플 수                              |
| **최소 샘플 리프 (min_samples_leaf)**  | 리프 노드가 되기 위한 최소 샘플 수                            |
| **최소 정보이득 (min_impurity_decrease)** | 분할 시 요구되는 최소 불순도 감소량                            |

### 2️⃣ 사후 가지치기 (Post-Pruning)

| 기법                                | 설명                                                                 |
|-------------------------------------|----------------------------------------------------------------------|
| **비용 복잡도 가지치기 (Cost-Complexity Pruning, CCP)** |  
\[
R_\alpha(T) = R(T) + \alpha\,|\text{leaves}(T)|
\]
  - \(R(T)\): 트리의 리프 노드 불순도 합  
  - \(\alpha\): 복잡도 페널티 → 교차검증으로 최적 \(\alpha\) 선택 |
| **가지치기-최소화 에러 차이 (Reduced Error Pruning)** | 검증 데이터에서 분리 후, 오차 증가가 없으면 해당 서브트리 제거     |

---

## 💡 실무 팁 요약

1. **지표 선택**  
   - 분류: 엔트로피(정보이득) vs 지니 지수(속도)  
   - 회귀: MSE vs MAE  
2. **전처리**  
   - **범주형** → Label/One-Hot 인코딩  
   - **결측치** 제거/대체 후 트리 학습  
3. **사전 가지치기**로 빠른 과적합 제어  
4. **CCP(사후 가지치기)**로 복잡도–성능 균형 조정  
5. **교차검증**으로 하이퍼파라미터(예: \(\alpha\), max_depth) 튜닝  
6. **앙상블**: Random Forest, Gradient Boosting으로 안정성·성능 ↑  

